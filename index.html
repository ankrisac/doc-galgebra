<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <title> Geometric Algebra </title>  

  <link href="https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;400;500;600;700&display=swap" rel="stylesheet">   <link href="style.css" rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>

</head>
<body>
  <script src="DOM.js"></script>
  <script src="vector.js"></script>
  <header> Geometric Algebra </header>

  <article>
    <header> 
      <h2> \(\mathbb{C}\) : "Imaginary" Numbers </h2>
      <p> 
        Or, why would anyone invent new numbers to 
        solve an equation no one asked the answer for 
      </p>
    </header>

    <div class="content">
      <p>
        To understand Complex numbers, we first need 
        to rewind back to 1545. The standard story for its 
        invention, involves finding the solution to
        $$x^2 + 1 = 0$$
        
        Which supposedly is continuing the tradition of 
        solving equations such as
        $$
        \begin{aligned}
        2x = 3 && \Rightarrow &&& \mathbb{Q^+} 
        \\ x^2 = 2 && \Rightarrow &&& \mathbb{R^+}
        \\ x + 4 = 0 && \Rightarrow &&& \mathbb{Z}
        \\ 5x = -3 && \Rightarrow &&& \mathbb{Q} 
        \end{aligned}
        $$
        
        However, this viewpoint ignores the fact that all the
        previous equations had some form of physical interpretation.  
      </p>

      <p>
        The actual origins of Complex numbers began with cubic equations.
        $$ax^3 + bx^2 + cx + d = 0$$   

        The solutions to cubic equations had been an unsolved problem
        for millenia, and it stumped many as to how the general cubic 
        equation was so much harder to solve than the general quadratic 
        equation. It took until the 16th century, when the solution 
        to the reduced cubic equation was finally found by Tartigali
        
        $$x^3 + px + q = 0$$
        $$
        \begin{aligned}
        D &= \left(\frac q2\right)^2 + \left(\frac{p}{3}\right)^3
        \\ C_+, C_- &= \sqrt[3]{-\frac q2 \pm \sqrt D}
        \\ x &= C_+ + C_-
        \end{aligned}
        $$
        
        However, this equation had some peculiar properties, 
        for instance, if one were to solve by graphing or by 
        trial and error
        
        $$x^3 - 6x - 4 = 0$$
        $$x = -2, 1 \pm \sqrt 3$$
        In contrast, using the cubic equation yields
        $$
        
        \begin{aligned}
        x = \sqrt[3]{2 - \sqrt{-4}} + \sqrt[3]{2 + \sqrt{-4}}
        \end{aligned}
        $$
        
        But if one treated these square roots of negative
        numbers <em>algebraicly</em> and simplified them, they 
        would yield <em>the same solutions</em> as above.
      </p>

      <p>
        This suggested that there was something deeper, and was
        the mystery that sparked the invention of complex numbers
      </p>
    </div>
  </article>


  <article>
    <header>
      <h2> \(a \times b\) : Cross ... Product? </h2>
      <p> 
        Why are right handed people suddenly fundamental 
        to the structure of the universe ? 
      </p>
    </header> 
    
    <div class="content">
      <p>
        Prior to the invention of vectors, mathematicans and physicists 
        needed a way to manipulate 3D coordinates in the same manner 
        as complex numbers did for 2D, as manipulating equations in
        component form was cumbersome. This was the problem that 
        Hamilton decided to tackle, and he tried for many years in vain.
      </p>

      <p>
        His first insight was that he would need to add a new
        imaginary constant \(j\). This was motivated by the fact that
        with complex numbers
        $$
        \begin{aligned}
        (-1)^2 = 1 && i^2 = 1
        \end{aligned}
        $$
        there were two axes that squared to one. However, unlike 
        \(\mathbb{C}\), his new system would have to remove commutativity, 
        otherwise it would be possible to equate \(j\) and \(i\). 
        This was not sufficent though, as many simple geometric theorems 
        did not hold. 
      </p>

      <p>
        Years later in 1843, he realized that he needed <em> three </em> 
        imaginary constants \(i, j, k\), (thus constructing a 4D system),
        in order to do 3D arithmetic. Additionally, he set \(ijk = -1\)
        to create a relation between the constants.
        
        $$i^2 = j^2 = k^2 = ijk = -1$$
        $$q = \underbrace{a}_\text{scalar} 
            + \underbrace{bi + cj + dk}_\text{vector}$$
        
        These rules enabled quaternions to be more powerful than 
        modern vector algebra (which was yet to be invented), with 
        addition, subtraction being identical to complex numbers, but 
        even multiplication and division were both possible 
        (unlike with vectors).

        $$
        \begin{aligned}
             z^{-1} &= \frac {z^*} {|z|^2} 
          = \frac
            {
              \overbrace{\left(a - bi\right)}
              ^{\mathbb{C}\text{ conjugate}}
            }
            {a^2 + b^2}
             
          \\ q^{-1} &= \frac {q^*} {|q|^2} 
          = \frac{
              \overbrace{\left(a - bi - cj - dk\right)}
              ^{\mathbb{H}\text{ conjugate}}
            }
            {a^2+b^2+c^2+d^2}
             
        \end{aligned}
        $$
      </p>
      <p>
        Multiplication of two purely imaginary quaternions 
        and simplification yielded
        
        $$
        \begin{aligned}
             AX =& (ai+bj+ck)(xi+yj+zk)  
          \\ AX =& -(ax + by + cz) 
          \\ & +(bz - cy)i - (az-cx)j + (ay - bx)k
        \end{aligned}
        $$
        
        The above expression upon inspection appears to be 
        composed of a dot product and cross product
        $$\vec{A}\;\vec{X} = \vec{A} \times \vec{X} - \vec{A} \cdot \vec{X}$$

        This is no coincidence, as <em> this </em> is the 
        origin of the two operators. Quaternions, as powerful
        as it was, were incredibly difficult to visualize. His 
        contemporaries, Gibbs & Heaviside attempted to remove the 
        two operators from their original context, and created 
        the right hand rule to replicate the quaternion rules, 
        thus creating modern vector analysis.

        And if we pay further attention, we can see the historical 
        relics of quaternions in their modern vector notation

        $$
        \begin{pmatrix} a \\ b \\ c \end{pmatrix} 
          = a\hat{\textbf{i}} + b\hat{\textbf{j}} + c\hat{\textbf{k}} 
          \iff a + bi + cj + dk 
        $$

        The right hand rule similarly originates from here, as it
        was an arbitrary adhoc explanation added on top of vectors to 
        simulate the multiplication rules of purely imaginary quaternions.
      </p>

      <p>
        As such, the cross product is an adhoc mismatch of partial 
        quaternion multiplication forced into 3D geometry, and falls
        apart outside of 3D geometry.
      </p>
    </div>
  </article>

  <article>
    <header> 
      <h2> \(a\wedge b\) : Wedge Product! </h2> 
      <p> Out with the new and ... in with the old ? </p> 
    </header>
    
    <div class="content">
      <p>
        Simuntaneously as Hamilton was developing his quaternions, Grassmann
        was developing his theory of exterior algebra and multivectors
      </p>

      <p>
        Exterior algebra extends the notion of vectors from simply oriented lengths,
        to more general measures such areas, volumes etc, each represented as
        bivectors, trivectors, etc respectively
      </p>
      <p>
        We shall now investigate the properties of the 2D wedge product on 
        bivectors. All the properties mentioned will hold in 
        higher dimensional bivectors, trivectors, etc.
      </p>
      
      <div id="fig-alt"></div>
      <script>
        {
          let Δ = new Diagram($("#fig-alt"), 400, 400);
          Δ.scale = 10;
          let cons = x => { x.P.norm().scl(5) };
    
          let col_A = $.css_get("--high-B");
          let col_B = $.css_get("--high-C");
    
          let O = Δ.point(0, 0);
          O.stroke = "black";
          O.fill = "black";

          let A = Δ.drag_point(8, 0);
          
          let B = Δ.drag_point(0, 8);
          B.fill = col_A;
          B.stroke = col_A;
    
          Δ.line(O, A);
          Δ.line(O, B).stroke = col_A;
    
          let C = Δ.point(8, 8).set_constraint(
            x => { x.P = A.P.cadd(B.P); }
          );
          C.stroke = col_B;
          C.fill = col_B;
          
          let p= Δ.plane(O, A, B, C);
          p.fill = col_B;
    
          Δ.build();
        }
      </script>
    
      <p>
        Using the above diagram, we can show that
        $$a\wedge a = 0$$
      </p>

      <div id="fig-commute"></div>
      <script>
        {
          let Δ = new Diagram($("#fig-commute"), 400, 400);
          Δ.scale = 10;
          let cons = x => { x.P.norm().scl(5) };
    
          let col_A = $.css_get("--high-B");
          let col_B = $.css_get("--high-C");
    
          let O = Δ.point(0, 0);
          O.stroke = "black";
          O.fill = "black";

          let A = Δ.drag_point(-4, 2);
          A.fill = col_A;
          A.stroke = col_A;
    
          let B = Δ.drag_point(0, 8).set_constraint(x => { x.P.norm().scl(9); });
          let C = Δ.drag_point(4, 0).set_constraint(x => { x.P.norm().scl(4); });
          
          let Q = Δ.point(0, 4).set_constraint(x => { x.P = A.P.cadd(C.P); });
          let R = Δ.point(4, 8).set_constraint(x => { x.P = B.P.cadd(C.P); });
    
          Δ.line(O, A).stroke = col_A;
    
          Δ.line(O, B);
          Δ.line(O, C);
          Δ.line(B, R);
          Δ.line(C, R);
          
          let pA = Δ.plane(O, A, C, Q);
          pA.fill = col_A;
          
          let pB = Δ.plane(A, B, Q, R);
          pB.fill = col_B;
    
          Δ.build();
        }
      </script>

      <p>
        Using the above diagram, we can show that
        $$
        \begin{aligned} 
        a\wedge (b + c) &= a \wedge b + a \wedge c
        \\ a\wedge b &= -b\wedge a
        \end{aligned}
        $$
      </p>
      <p>
        And by using the above properties, we can prove the following
      
        $$
        \begin{aligned} 
        k(a\wedge b) &= (ka)\wedge b = a \wedge (kb)
        \\ a\wedge b &= a \wedge (b + ka)
        \end{aligned}
        $$
      </p>

      <p>
        The properties shown above generalize to higher dimensions
        easily, and thus apply to trivectors, and other higher 
        dimensional vectors aswell.  
      </p>
    
    </div>
  </article>

  <article>
    <header> 
      <h2> \(det(A)\) : Determinants </h2> 
      <p> But determining what ? </p> 
    </header>

    <div class="content">
      <p>
        One of the most useful aspects of the wedge product is 
        to motivate the theory of determinants.
      </p>  
      
      <p>
        The determinant, is usually introduced as simply a formula
        $$\begin{vmatrix} a&b \\ c&d\end{vmatrix} = ad - bc$$
        And an algorithm (Laplace's expansion) is used 
        to defined higher order determinants
        $$
        \begin{vmatrix} a&b&c \\ d&e&f \\ g&h&i \end{vmatrix} = 
        + a \begin{vmatrix} e & f \\ h & i \end{vmatrix}
        - b \begin{vmatrix} d & f \\ g & i \end{vmatrix}
        + c \begin{vmatrix} d & e \\ g & h \end{vmatrix}
        $$
        Or alternatively it may be defined as the unique multilinear 
        alternating function, from which all of its properties may
        be derived, but this approach does not provide much 
        geometric insight.
      </p>

      <p>
        However, using the wedge product, a simple geometric 
        interpretation is possible
        $$
        \begin{vmatrix} 
             a_1    & b_1    & \dots  & z_1 
          \\ a_2    & b_2    & \dots  & z_2
          \\ \vdots & \vdots & \ddots & \vdots 
          \\ a_n    & b_n    & \dots  & z_n
        \end{vmatrix}
        = \frac{\vec{a} \wedge \vec{b} \wedge \dots \wedge \vec{z}}
          {i \wedge j \wedge \dots}
        = (\vec{a} \wedge \vec{b} \wedge \dots \wedge \vec{z})I^{-1}
        $$
        Where \(I\) is the N dimensional unit pseudoscalar/volume 
      </p>

      <p>
        In short, the determinant represents a generalization 
        of areas, and volumes to N dimensions.
        For instance in 2D, it is the area of a parallelogram spanned by 2 vectors.
        And in 3D, it is the volume of a parallelopiped spanned by 3 vectors.
        
      </p>

      <p>
        Many properties of the wedge product 
        $$
        \begin{aligned}
           a \wedge a    &= 0
        \\ k(a \wedge b) &= (ka) \wedge b     
        \\ a \wedge b    &= -b \wedge a
        \\ a \wedge b    &= a \wedge b + k(b \wedge b) = (a + kb)\wedge b 
        \end{aligned}
        $$
        thus are also inherited by the determinant
        $$
        \begin{aligned}
           \det(\dots, a, \dots, a, \dots) &= 0
        \\ \det(\dots, ka, \dots)          &= k \det(\dots, a, \dots)            
        \\ \det(\dots, a, b, \dots)        &= -\det(\dots, b, a, \dots)            
        \\ \det(\dots, a, \dots, b, \dots) &= \det(\dots, a, \dots, b + ka, \dots)
        \end{aligned}
        $$
      </p>

      <aside>
        <header>
          The Leibniz determinant formula also follows from the wedge 
          product definition, and can be used to prove
          $$\det A^T = \det A$$ 
        </header>
        <div class="max"> 
          <p>
            For a \(N\times N\) matrix \(A\), instead of expanding one column, we completely expand
            every column simuntaneously. 
          </p>
          
          $$
          \begin{aligned}
          \det(A)I &= 
                 \begin{pmatrix} a\\\vdots\\\phantom{x} \end{pmatrix}
          \wedge \begin{pmatrix} \phantom{x}\\\vdots\\b \end{pmatrix}
          \wedge \begin{pmatrix} \vdots\\c\\\vdots \end{pmatrix}
          \\ &= \dots + abc(i\wedge k\wedge j) + \dots
          \\ &= (\dots -acb + \dots)(i\wedge j \wedge k)
          \end{aligned}
          $$

          <p>
            Thus the expansion yields the sum of every possible
            combination of wedge products terms, taking one 
            component from each column. 
            And due to repeated basis vectors equaling zero, 
            the components are only from unique rows. 
          </p>
            
          <p>
            The swapping procedure to convert the basis vectors to
            standard form is formally denoted using the parity 
            function \(\sigma(\tau)\),
            
            $$\sigma(\tau) = (-1)^N$$
            where \(N\) is the number of swaps required to convert 
            to standard form, for a given permutation function \(\tau\).
            For example
            $$
            \begin{aligned}
            \tau = (132) 
            &= \begin{cases} 
                 \tau(1) = 1 
              \\ \tau(2) = 3 
              \\ \tau(3) = 2
            \end{cases}
            \\ \sigma(\tau) = \sigma(132) 
               &= i\wedge \underbrace{k \wedge j}_{N = \text{1 swap}} 
            \\ &= (-1)^1(i\wedge j\wedge k) 
            \\ &= -\sigma(123)
            \end{aligned}
            $$
          </p>

          <p>
            Thus we can rephrase the above method as, 
            \(\det(A_{N\times N})\) is the sum of every possible 
            wedge product taking \(N\) components such that no 
            row or column is repeated.
            $$
              \det(A) = 
              \underbrace{\sum_{\tau \in S_n}}_\text{sum all combinations}
              \overbrace{\sigma(\tau)}^\text{correct the sign}
              \underbrace{\prod_{i = 1}^N a_{i, \tau(i)}}_\text{multiply components}
            $$
            Where \(S_n\) is the set of all permutation functions
            on \((1,\dots N)\).
            Thus we obtain the Leibniz formula for determinants
            $$
              \det(A) = \sum_{\tau \in S_n}
              \sigma(\tau)\prod_{i = 1}^N a_{i, \tau(i)}
            = \sum_{\tau \in S_n} \det(D_\tau)
            $$
          </p>

          <p>
            By inspecting our procedure for standardizing any 
            permutation \(\tau\), we see that it is equivalent to 
            diagonalizing the <em></em> matrix \(D_\tau\) given below

            $$
            \begin{aligned}
            det(D_\tau)I &\coloneqq acb(i\wedge k\wedge j) 
            \\          &= -abc(i\wedge j\wedge k)
            \\ \det(D_\tau) &= \begin{vmatrix} 
                 a &   &  
              \\   &   & b
              \\   & c &  
              \end{vmatrix}
            \to \begin{vmatrix} 
                 a &   &   
              \\   & b &  
              \\   &   & c 
              \end{vmatrix}
            & \text{(column swap)}
            \\ \det(D^T_\tau) &= \begin{vmatrix} 
                 a &   &  
              \\   &   & c
              \\   & b &  
              \end{vmatrix}
            \to \begin{vmatrix} 
                 a &   &   
              \\   & b &  
              \\   &   & c 
              \end{vmatrix}
            & \text{(row swap)}
            \end{aligned}
            $$
            But in this format, every column swap on \(D\)
            corresponds to a row swap in \(D^T\). We can express 
            every row swap by relabelling two <em> basis 
            vectors </em>, for example
            $$
            \begin{aligned}
            \det(D_\tau)I
            &=       \begin{pmatrix} +ai\\+0j\\+0k \end{pmatrix}
              \wedge \begin{pmatrix} +0i\\+0j\\+bk \end{pmatrix}
              \wedge \begin{pmatrix} +0i\\+cj\\+0k \end{pmatrix}
            \\ &=       
                     \begin{pmatrix} +ai\\+0j\\+0k \end{pmatrix}
              \wedge \begin{pmatrix} +0i\\+bj\\+0k \end{pmatrix}
              \wedge \begin{pmatrix} +0i\\+0j\\+ck \end{pmatrix}
              & (j, k\to k, j)
            \\ &= abc(i\wedge j\wedge k) 
            \\ &= abc(i\wedge k\wedge j) & (k, j\to j, k)
            \\ &= -abc(i\wedge j\wedge k)
            \end{aligned}
            $$
            But as seen from above,
            row relabeling in \(D^T\) is equivalent to 
            column swapping on \(D^T\). Thus combining the two
            previous inferences, column swapping on \(D\) 
            is equivalent to column swapping on \(D^T\).
            Hence,
            $$
            \begin{aligned}
            \det(A) &= \sum_{\tau\in S_n} \det(D_\tau) 
            \\      &= \sum_{\tau\in S_n} \det(D_\tau^T) 
                    = \det(A^T)
            \end{aligned}          
            $$
          </p>
        </div>
      </aside>
      
      <aside>
        <header> 
          And Laplace's expansion as mentioned earlier, simply becomes
          a specific manner of evaluating the wedge product 
        </header>
        
        <div class="max">
          $$
          \begin{vmatrix} a&d&g \\ b&e&h \\ c&f&i \end{vmatrix} = 
          (ai + bj + ck) \wedge (di + ej + fk) \wedge (gi + hj + ik)  
          $$
        
          <p>
            By expanding the first term and eliminating repeated basis
            vectors we obtain
          </p>
        
          $$
          \begin{aligned}
            = &+ ai \wedge (di + ej + fk) \wedge (gi + hj + ik) + \dots
          \\ = &+ \underbrace{ai \wedge di}_0 \wedge (gi + hj + ik)
          \\   &+ \underline{ai} \wedge (ej + fk) \wedge \underline{gi}
          \\   &+ ai \wedge (ej + fk) \wedge (hj + ik)
          \\   &+ \underbrace{bj \wedge (di + ej + fk)}
                  _\text{swap and expand} \wedge (gi + hj + ik)
          \\   &+ \dots
          \\ = &+a \begin{vmatrix} e & f \\ h & i \end{vmatrix} 
                -d \begin{vmatrix} b & h \\ c & i \end{vmatrix} 
                +g \begin{vmatrix} b & e \\ c & f \end{vmatrix}
          \end{aligned}
          $$      
          <p>
            Similarly, by using the swapping property of determinants,
            expansion on any other column is possible, as well as 
            generalizing it to N dimensions. Thus, for any column \(J\)
            $$
            \det(A_{M\times N}) = \sum_{i=1}^M (-1)^{i + J}det(M_{iJ})
            $$
            Where \(M_{ij}\) is a minor (subset of matrix \(A\)
            with row \(i\), and column \(j\) removed)
          </p>
        
          <p>
            We can also show that expansion on the row follows similarly, 
            due to the invariance of the determinant under transpose
            \((\det{A^T} = \det A\))
          </p>

        </div>
      </aside>
    <div>
  </article>

  <article>
    <header> 
      <h2> \(A^{-1}x\) : Cramer's Rule </h2> 
      <p> bolts of vectors </p> 
    </header>
    <div class="content">
      <p>
        Another equally useful application of wedge products
        is in solving system of N linear equations (SLEs)
        $$
        \begin{aligned}
          a_1x + b_1y + \dots &= Λ_1
        \\ a_2x + b_2y + \dots &= Λ_2
        \\        \vdots
        \\ a_nx + b_ny + \dots &= Λ_n
        \end{aligned}
        $$
        The above SLE can be reexpressed as a single 
        vector equation in N dimensions
        $$ax + by + \dots = Λ$$
      </p>
      <p> 
        By using the property \(a\wedge a = 0\), we can isolate
        a single variable as follows
        $$
        \begin{aligned}
        ax + by + \dots &= Λ
        \\ (a\wedge b)x + (b\wedge b)y + \dots &= Λ\wedge b
        \\ (a\wedge b)x + 0 + (c\wedge b)z + \dots &= Λ\wedge b
        \\ \vdots
        \\ (a\wedge b\wedge c \dots)x &= Λ\wedge b\wedge c\dots
        \\ x &= \frac{Λ\wedge b \wedge c \dots}{a\wedge b\wedge c\dots}
        \end{aligned}
        $$
        If we denote \((x,y,\dots\)) as a vector \(X\),
        \((a,b,\dots)\) as a matrix \(A\) then we obtain
        $$
        X = \frac{1}{a\wedge b\wedge c\dots}
        \begin{bmatrix}
          Λ\wedge b \wedge c \dots
        \\ a\wedge Λ \wedge c \dots
        \\ a\wedge b \wedge Λ \dots
        \\ \vdots
        \end{bmatrix}
        = \frac{1}{\det A}
        \begin{bmatrix}
           \det A_Λ(1)
        \\ \det A_Λ(2)
        \\ \det A_Λ(3)
        \\ \vdots
        \end{bmatrix}
        $$
        Where \(A_Λ(n)\) is the matrix
        \(A\), with the \(n^\text{th}\) column replaced by \(Λ\), Which 
        gives us cramers rule.
      </p>
      <aside>
        <header> 
          From the above result, it can be shown that the inverse of a matrix, is given by
          $$A^{-1} = \frac{\mathrm{adj}(A)}{\det A}$$
  
          
        </header>
        <div class="max">
          First note that
          $$AA^{-1} = I \to AA^{-1}_j = I_j = ε_j$$  

          Where \(ε_i\) is the \(i^\text{th}\) basis vector, and
          \(A_j\) is the \(j^\text{th}\) column        
          $$A^{-1}_j = \frac{1}{\det A}
          \begin{bmatrix}
             \det A_{ε_j}(1)
          \\ \det A_{ε_j}(2)
          \\ \vdots
          \end{bmatrix}
          $$
          As only the \(i^{th}\) component of \(ε_i\) is non zero, 
          we can use Laplace's expansion along column \(j\), yielding  
          $$\det A_{ε_j}(i) = (-1)^{i+j}\det M_{ij}$$
          Where \(M_{ij}\) is a minor of A. Thus in general,
          $$A^{-1}_{ij} = \frac{1}{\det A}(-1)^{i+j}\det M_{ij}$$
          If we define
          $$\mathrm{adj}(A) = (-1)^{i+j}\det M_{ij}$$
          We get the inverse matrix formula,
          $$A^{-1} = \frac{\mathrm{adj}(A)}{\det A}$$
  
        </div>
      </aside>
    </div>
  </article>

  <article>
    <header> 
      <h2> \(\mathcal{G}^n\) : Geometric Algebra </h2> 
      <p> Old beginnings ? </p> 
    </header>
  </article>

  <script>
    let prev = null;
    for(let node of $("article")) {
      $("header", node)[0].onclick = () => {
        node.classList.toggle("view");

        if(prev !== null && prev !== node){
          prev.classList.remove("view");
          for(let child of $("aside", prev)) {
            child.classList.remove("view");
          }
        }
        prev = node;
      } 
    }
    for(let node of $("aside")) {
      $("header", node)[0].onclick = () => 
        node.classList.toggle("view");
    }
  </script>

</body>
</html>